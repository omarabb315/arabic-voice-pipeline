# NeuTTS Arabic Voice Cloning Dataset Pipeline

Production-ready pipeline for building large-scale Arabic voice cloning datasets across multiple machines with different capabilities.

## ğŸ¯ Overview

This repository contains the complete workflow for:
1. **Stage 1 (GCP T4)**: Extract audio segments from videos with transcripts
2. **Stage 2 (H200)**: Add VQ codes using NeuCodec (GPU-accelerated)
3. **Final Step**: Create HuggingFace dataset and push to Hub

**Current Status**: 656K segments (221GB) processed across 3 channels âœ…

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GCP T4 VM     â”‚  Stage 1: Extract segments
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â€¢ Download from GCS
â”‚  Audio segments â”‚  â€¢ Segment based on transcripts
â”‚    221GB .npz   â”‚  â€¢ Save as .npz (audio + metadata)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Upload via boto3
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 Bucket     â”‚  Intermediate Storage
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â€¢ aifiles bucket (10TB)
â”‚  stage1/        â”‚  â€¢ Custom endpoint
â”‚  stage2_with_   â”‚  â€¢ 221GB â†’ 230GB with codes
â”‚    codes/       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Batch download (70K segments = 24GB)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  H200 Machine   â”‚  Stage 2: Add VQ codes
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â€¢ Batch processing (33GB available)
â”‚  NeuCodec GPU   â”‚  â€¢ Encode â†’ Upload â†’ Clean
â”‚  encoding       â”‚  â€¢ Resume capability
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Create HF dataset
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HuggingFace Hub â”‚  Final Dataset
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â€¢ ~60GB optimized format
â”‚  Ready for      â”‚  â€¢ Load from anywhere
â”‚  training       â”‚  â€¢ Public/private repo
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Repository Structure

```
neutts/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ config.yaml                  # Training configuration
â”œâ”€â”€ .gitignore                   # Git exclusions
â”‚
â”œâ”€â”€ scripts/                     # Main pipeline scripts
â”‚   â”œâ”€â”€ build_dataset_robust.py       # Stage 1: Extract segments (GCP)
â”‚   â”œâ”€â”€ s3_utils.py                   # S3 client utilities
â”‚   â”œâ”€â”€ upload_to_s3.py               # Upload segments to S3
â”‚   â”œâ”€â”€ add_codes_to_dataset_s3.py    # Stage 2: Add codes (H200)
â”‚   â”œâ”€â”€ download_from_s3.py           # Download from S3
â”‚   â””â”€â”€ add_codes_to_dataset.py       # Original (local processing)
â”‚
â”œâ”€â”€ training/                    # Training scripts
â”‚   â”œâ”€â”€ finetune-neutts.py           # Fine-tune NeuTTS
â”‚   â”œâ”€â”€ create_pairs.py              # Create training pairs
â”‚   â””â”€â”€ neutts.py                    # Model code
â”‚
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ DATASET_BUILD_WORKFLOW.md    # Detailed workflow
â”‚   â””â”€â”€ STAGE2_H200_SETUP.md         # H200 setup guide
â”‚
â””â”€â”€ create_hf_dataset_from_npz.py   # Create HF dataset (supports S3)
```

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.8+
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Stage 1: Build Segments (GCP T4)

```bash
# Extract segments from videos
python scripts/build_dataset_robust.py \
  --channels='["ZadTVchannel", "Atheer_-", "thmanyahPodcasts"]' \
  --skip_codec_encoding=True \
  --create_hf_dataset=False

# Upload to S3
export S3_ENDPOINT_URL="https://your-endpoint:9021"
export S3_ACCESS_KEY="your-access-key"
export S3_SECRET_KEY="your-secret-key"
export S3_BUCKET_NAME="aifiles"

python scripts/upload_to_s3.py \
  --source_dir=segments_cache/ \
  --s3_prefix=neutts_segments/stage1/
```

### Stage 2: Add VQ Codes (H200)

```bash
# Clone repo on H200
git clone https://github.com/your-username/neutts.git
cd neutts
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Set S3 credentials
export S3_ENDPOINT_URL="https://AJN-NAS-01.ALJAZEERA.TV:9021"
export S3_ACCESS_KEY="your-access-key"
export S3_SECRET_KEY="your-secret-key"
export S3_BUCKET_NAME="aifiles"

# Process in batches
python scripts/add_codes_to_dataset_s3.py \
  --s3_prefix=neutts_segments/stage1/ \
  --output_prefix=neutts_segments/stage2_with_codes/ \
  --batch_size=70000 \
  --codec_device=cuda
```

**Options:**
- `--batch_size=70000` â†’ ~24GB per batch (recommended, 72% of 33GB)
- `--batch_size=50000` â†’ ~17GB per batch (safer, 51% of 33GB)

### Final Step: Create HuggingFace Dataset

**Option A: On H200 (streaming from S3)**
```bash
# Login to HuggingFace
huggingface-cli login

# Create and push dataset
python create_hf_dataset_from_npz.py \
  --s3_prefix=neutts_segments/stage2_with_codes/ \
  --output_path=/tmp/segments_dataset/ \
  --push_to_hub=True \
  --hf_repo=omarabb315/arabic-voice-cloning-dataset
```

**Option B: On GCP T4 (download first)**
```bash
# Download encoded segments
python scripts/download_from_s3.py \
  --s3_prefix=neutts_segments/stage2_with_codes/ \
  --output_dir=segments_cache_encoded/

# Create dataset
python create_hf_dataset_from_npz.py \
  --segments_cache_dir=segments_cache_encoded/ \
  --output_path=dataset_cache/final/ \
  --push_to_hub=True \
  --hf_repo=omarabb315/arabic-voice-cloning-dataset
```

### Use for Training

```python
from datasets import load_dataset

# Load from HuggingFace Hub
dataset = load_dataset("omarabb315/arabic-voice-cloning-dataset")

# Ready for training!
# Each sample contains:
# - audio: waveform (16kHz)
# - codes: VQ codes for training
# - text: transcription
# - speaker_id, channel, gender, dialect, tone, duration_sec
```

## ğŸ“Š Current Dataset Stats

| Channel | Segments | Duration | Size |
|---------|----------|----------|------|
| ZadTVchannel | 279,000 | ~93 hours | 94 GB |
| Atheer - Ø£Ø«ÙŠØ± | 171,000 | ~57 hours | 58 GB |
| Thmanyah Podcasts | 205,000 | ~68 hours | 69 GB |
| **Total** | **656,300** | **~218 hours** | **221 GB** |

## ğŸ”§ Key Features

### Batch Processing
- **Smart batching**: Process 70K segments at a time (~24GB)
- **Resume capability**: Automatically resumes after interruptions
- **Progress tracking**: JSON-based checkpoint system

### S3 Integration
- **Custom endpoint support**: Works with any S3-compatible storage
- **Parallel uploads**: Multi-threaded for speed
- **Progress bars**: Real-time transfer monitoring

### HuggingFace Hub
- **Direct push**: Upload dataset to Hub from anywhere
- **Streaming support**: Create dataset without downloading all files
- **Public/private**: Choose visibility

## ğŸ“ Environment Variables

### S3 Configuration (Required for Stage 2)
```bash
export S3_ENDPOINT_URL="https://your-endpoint:9021"
export S3_ACCESS_KEY="your-access-key"
export S3_SECRET_KEY="your-secret-key"
export S3_BUCKET_NAME="aifiles"
```

### GCS Configuration (Required for Stage 1)
```bash
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account-key.json"
```

### HuggingFace (Required for Hub upload)
```bash
# Login once
huggingface-cli login
```

## ğŸ› Troubleshooting

### S3 Connection Issues
```bash
# Test connection
python -c "from scripts.s3_utils import test_connection; test_connection()"
```

### Out of Space on H200
```bash
# Check disk usage
df -h /tmp

# Use smaller batches
python scripts/add_codes_to_dataset_s3.py --batch_size=50000
```

### Resume After Interruption
```bash
# Just re-run the same command - it automatically resumes!
python scripts/add_codes_to_dataset_s3.py \
  --s3_prefix=neutts_segments/stage1/ \
  --output_prefix=neutts_segments/stage2_with_codes/
```

## ğŸ“š Documentation

- [**DATASET_BUILD_WORKFLOW.md**](docs/DATASET_BUILD_WORKFLOW.md) - Complete workflow guide
- [**STAGE2_H200_SETUP.md**](docs/STAGE2_H200_SETUP.md) - H200 setup instructions

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License.



