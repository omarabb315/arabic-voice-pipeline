# Orpheus Voice Cloning Full Fine-tuning Configuration
# This config uses FULL fine-tuning (all parameters trained)
# Requires significantly more GPU memory than LoRA

# Model settings
restore_from: "unsloth/orpheus-3b-0.1-ft"  # or path to your checkpoint
max_seq_len: 4096

# Dataset settings
paired_dataset_path: "dataset_cache/paired_dataset/"

# Training settings
save_root: "checkpoints/"
run_name: "orpheus-arabic-voice-cloning-full-ft"

# Hyperparameters (adjusted for full fine-tuning)
lr: 5.0e-5  # Lower learning rate for full fine-tuning
per_device_train_batch_size: 2  # Smaller batch size (more memory needed)
gradient_accumulation_steps: 16  # Increase to maintain effective batch size
num_train_epochs: 3
max_steps: -1  # -1 means use num_train_epochs

# LoRA settings - DISABLED for full fine-tuning
use_lora: false  # Set to true for LoRA (memory efficient)
lora_r: 64  # Ignored when use_lora is false
lora_alpha: 64  # Ignored when use_lora is false
load_in_4bit: false  # Full precision (4bit not recommended for full fine-tuning)

# Optimization
warmup_ratio: 0.1  # More warmup for full fine-tuning
weight_decay: 0.01
lr_scheduler_type: "cosine"

# Checkpointing
save_steps: 500
logging_steps: 1
save_total_limit: 2

# Preprocessing
num_preprocessing_workers: 40

# Weights & Biases
wandb_project: "Orpheus-Arabic-Voice-Cloning-Full-FT"

# Note: Full fine-tuning memory requirements:
# - Minimum: 80GB GPU (A100/H100)
# - Recommended: 2x A100 80GB or H100
# - May require gradient checkpointing and optimization

